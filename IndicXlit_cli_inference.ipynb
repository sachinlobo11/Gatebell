{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinlobo11/Gatebell/blob/main/IndicXlit_cli_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_h7A8b0FVbx"
      },
      "source": [
        "### Faiseq Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0tRbUVEjIvWT",
        "outputId": "b91f8cab-e120-4e56-f6ea-154b53f4c174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (7.4.7)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (0.20.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
            "Requirement already satisfied: alabaster~=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
            "fatal: destination path 'fairseq' already exists and is not an empty directory.\n",
            "/content/fairseq\n",
            "Collecting pip==23\n",
            "  Downloading pip-23.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Downloading pip-23.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-23.0\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.17.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.1+cu121)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.11)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.26.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.9.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.3.2)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.1+cu121)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2024.6.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9552 sha256=ac9f64ba175054766a6e241dc61831f3eab21f2a4b416d94c95b4d2074d4652f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b1j8zopq/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141213 sha256=1a1a14f847cb0fb084c3535a8da3b419bd72b2e5275c33405243702adb1bb7cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "# !pip3 install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library xformers triton\n",
        "!pip3 install indic-nlp-library\n",
        "\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/facebookresearch/fairseq.git\n",
        "%cd fairseq\n",
        "!pip install --upgrade pip==23\n",
        "!pip install --editable ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4OTyejVhG27h"
      },
      "outputs": [],
      "source": [
        "# adding fairseq path to env variable\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone IndicXlit repository"
      ],
      "metadata": {
        "id": "Nlm02wogxwGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the IndicXlit repository (to setup and run command line inference)\n",
        "!git clone https://github.com/AI4Bharat/IndicXlit.git"
      ],
      "metadata": {
        "id": "zApgmHsYxtL-",
        "outputId": "08f085cd-572f-45b9-c74d-8f8c51b809a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IndicXlit'...\n",
            "remote: Enumerating objects: 1197, done.\u001b[K\n",
            "remote: Counting objects: 100% (547/547), done.\u001b[K\n",
            "remote: Compressing objects: 100% (279/279), done.\u001b[K\n",
            "remote: Total 1197 (delta 344), reused 426 (delta 260), pack-reused 650 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1197/1197), 3.94 MiB | 9.03 MiB/s, done.\n",
            "Resolving deltas: 100% (562/562), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# En-Indic setup"
      ],
      "metadata": {
        "id": "gG-S1hiFqgtO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zx1P4hX5Dp2"
      },
      "source": [
        "### Download the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r7Deb46sOQ3f",
        "outputId": "4887bf33-39c3-46ff-a0ca-3c207fb8b51a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IndicXlit/inference/cli/en-indic\n",
            "--2024-09-20 16:17:05--  https://github.com/AI4Bharat/IndicXlit/releases/download/v1.0/indicxlit-en-indic-v1.0.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/487173539/4ef3b62d-385b-4a3a-9ab1-a3cc55764ef3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240920%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240920T161705Z&X-Amz-Expires=300&X-Amz-Signature=34013aa2314adc2ba18919fcd4774b2572c2c4a814e56b851e4adb46cd2320b6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dindicxlit-en-indic-v1.0.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-09-20 16:17:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/487173539/4ef3b62d-385b-4a3a-9ab1-a3cc55764ef3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240920%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240920T161705Z&X-Amz-Expires=300&X-Amz-Signature=34013aa2314adc2ba18919fcd4774b2572c2c4a814e56b851e4adb46cd2320b6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dindicxlit-en-indic-v1.0.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 126832968 (121M) [application/octet-stream]\n",
            "Saving to: ‘indicxlit-en-indic-v1.0.zip’\n",
            "\n",
            "indicxlit-en-indic- 100%[===================>] 120.96M   121MB/s    in 1.0s    \n",
            "\n",
            "2024-09-20 16:17:07 (121 MB/s) - ‘indicxlit-en-indic-v1.0.zip’ saved [126832968/126832968]\n",
            "\n",
            "Archive:  indicxlit-en-indic-v1.0.zip\n",
            "   creating: corpus-bin/\n",
            "  inflating: corpus-bin/dict.as.txt  \n",
            "  inflating: corpus-bin/dict.bn.txt  \n",
            "  inflating: corpus-bin/dict.brx.txt  \n",
            "  inflating: corpus-bin/dict.en.txt  \n",
            "  inflating: corpus-bin/dict.gom.txt  \n",
            "  inflating: corpus-bin/dict.gu.txt  \n",
            "  inflating: corpus-bin/dict.hi.txt  \n",
            "  inflating: corpus-bin/dict.kn.txt  \n",
            "  inflating: corpus-bin/dict.ks.txt  \n",
            "  inflating: corpus-bin/dict.mai.txt  \n",
            "  inflating: corpus-bin/dict.ml.txt  \n",
            "  inflating: corpus-bin/dict.mlt.txt  \n",
            "  inflating: corpus-bin/dict.mni.txt  \n",
            "  inflating: corpus-bin/dict.mr.txt  \n",
            "  inflating: corpus-bin/dict.ne.txt  \n",
            "  inflating: corpus-bin/dict.or.txt  \n",
            "  inflating: corpus-bin/dict.pa.txt  \n",
            "  inflating: corpus-bin/dict.sa.txt  \n",
            "  inflating: corpus-bin/dict.si.txt  \n",
            "  inflating: corpus-bin/dict.sd.txt  \n",
            "  inflating: corpus-bin/dict.ta.txt  \n",
            "  inflating: corpus-bin/dict.ur.txt  \n",
            "  inflating: corpus-bin/dict.te.txt  \n",
            "   creating: transformer/\n",
            "  inflating: transformer/indicxlit.pt  \n",
            "--2024-09-20 16:17:09--  https://github.com/AI4Bharat/IndicXlit/releases/download/v1.0/word_prob_dicts.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/487173539/4b1b0b15-1996-4092-b33e-10d30761e65c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240920%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240920T161709Z&X-Amz-Expires=300&X-Amz-Signature=aa21dffded0b98a9c3f50eff9030c74dcf63518623acdcc607d4c291c4fc20d6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dword_prob_dicts.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-09-20 16:17:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/487173539/4b1b0b15-1996-4092-b33e-10d30761e65c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240920%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240920T161709Z&X-Amz-Expires=300&X-Amz-Signature=aa21dffded0b98a9c3f50eff9030c74dcf63518623acdcc607d4c291c4fc20d6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dword_prob_dicts.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 850493322 (811M) [application/octet-stream]\n",
            "Saving to: ‘word_prob_dicts.zip’\n",
            "\n",
            "word_prob_dicts.zip 100%[===================>] 811.09M   112MB/s    in 6.9s    \n",
            "\n",
            "2024-09-20 16:17:16 (118 MB/s) - ‘word_prob_dicts.zip’ saved [850493322/850493322]\n",
            "\n",
            "Archive:  word_prob_dicts.zip\n",
            "   creating: word_prob_dicts/\n",
            "  inflating: word_prob_dicts/si_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/pa_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/brx_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/bn_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/ne_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/gom_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/ta_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/sd_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/mai_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/kn_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/sa_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/te_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/ur_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/hi_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/mni_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/gu_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/mr_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/or_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/ks_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/as_word_prob_dict.json  \n",
            "  inflating: word_prob_dicts/ml_word_prob_dict.json  \n"
          ]
        }
      ],
      "source": [
        "# to setup and run en-indic command line inference\n",
        "%cd /content/IndicXlit/inference/cli/en-indic\n",
        "\n",
        "# download the IndicXlit models from github release\n",
        "!wget https://github.com/AI4Bharat/IndicXlit/releases/download/v1.0/indicxlit-en-indic-v1.0.zip\n",
        "!unzip indicxlit-en-indic-v1.0.zip\n",
        "\n",
        "# download the Unigram dictionaries for reranking from github release\n",
        "!wget https://github.com/AI4Bharat/IndicXlit/releases/download/v1.0/word_prob_dicts.zip\n",
        "!unzip word_prob_dicts.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ef_VAVUM6f44",
        "outputId": "e28b4aaf-0a21-4e49-c293-e880c371f7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘source’: File exists\n",
            "mkdir: cannot create directory ‘output’: File exists\n"
          ]
        }
      ],
      "source": [
        "# creating the dir for saving input files\n",
        "%mkdir source\n",
        "\n",
        "# creating the dir for saving output files\n",
        "%mkdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp2g35mx7nyb"
      },
      "source": [
        "### Saving the input words to source file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "12BginQM6tKU"
      },
      "outputs": [],
      "source": [
        "# Preparing a list of words, format : space separated roman characters in each line\n",
        "!echo \"h a n v\" >> source/source.txt\n",
        "!echo \"m a g n n e m\" >> source/source.txt\n",
        "!echo \"k o r t a m\" >> source/source.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PLxuVb56SwD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input file would look like this\n",
        "```\n",
        "b h a r a t\n",
        "i n d i a\n",
        "h a i\n",
        "````"
      ],
      "metadata": {
        "id": "p2jvbhbJMYno"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwjEdHpQ7tl-"
      },
      "source": [
        "### Running the inference script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Gf3eT7CoOto_",
        "outputId": "cfa53bbf-3389-4bad-bd1d-eb3ea9b8c6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-20 17:00:44.931541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-20 17:00:44.952286: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-20 17:00:44.958664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-20 17:00:44.974543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-20 17:00:46.123717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-09-20 17:00:47 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-09-20 17:00:50 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'transformer/indicxlit.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'beam_mt': 0, 'nbest': 5, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'source/source.txt'}, 'model': None, 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='transformer/indicxlit.pt', post_process=None, quiet=False, model_overrides='{}', results_path=None, beam=10, beam_mt=0, nbest=5, max_len_a=0, max_len_b=200, max_len_a_mt=0, max_len_b_mt=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, lenpen_mt=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=0.0, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, eos_token=None, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, buffer_size=0, input='source/source.txt', source_lang='en', target_lang='kn', lang_pairs='en-as,en-bn,en-brx,en-gom,en-gu,en-hi,en-kn,en-ks,en-mai,en-ml,en-mni,en-mr,en-ne,en-or,en-pa,en-sa,en-sd,en-si,en-ta,en-te,en-ur', keep_inference_langtok=False, sampling_method='concat', sampling_temperature=1.5, data='corpus-bin', langs=None, lang_dict='lang_list.txt', source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=False, encoder_langtok='tgt', decoder_langtok=False, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2024-09-20 17:00:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | loaded language list from lang_list.txt as they are ordered in file\n",
            "2024-09-20 17:00:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 54 types\n",
            "2024-09-20 17:00:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | [kn] dictionary: 806 types\n",
            "2024-09-20 17:00:50 | INFO | fairseq_cli.interactive | loading model(s) from transformer/indicxlit.pt\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-09-20 17:00:51 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-09-20 17:00:51 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=1684.10546875Mb; avail=10961.171875Mb\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000672\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=1684.10546875Mb; avail=10961.171875Mb\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000043\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=1684.10546875Mb; avail=10961.171875Mb\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000866\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.002236\n",
            "2024-09-20 17:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=1684.10546875Mb; avail=10961.171875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2005.30859375Mb; avail=10635.90234375Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000619\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2005.30859375Mb; avail=10635.90234375Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2005.30859375Mb; avail=10635.90234375Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000104\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001458\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2005.30859375Mb; avail=10635.90234375Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000787\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000078\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001605\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000615\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000033\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000088\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001406\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000595\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000034\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000079\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001388\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000568\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000083\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001355\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000452\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000028\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000075\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001190\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.29296875Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000626\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000078\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001456\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000551\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000080\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001305\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.91796875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000498\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000031\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000073\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001221\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000473\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000029\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000075\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001194\n",
            "2024-09-20 17:00:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000499\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000029\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000079\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001242\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.2890625Mb; avail=10634.921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.53515625Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000475\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.53515625Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000029\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.53515625Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000074\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001236\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.53515625Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000496\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000030\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000073\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001251\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000483\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000028\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000077\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001218\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000476\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000027\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000065\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001184\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000468\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000028\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000071\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001172\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000542\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000073\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001351\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000727\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000034\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000082\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001700\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2006.52734375Mb; avail=10634.67578125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2010.46484375Mb; avail=10630.73828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000761\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2010.7109375Mb; avail=10630.4921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000034\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2010.7109375Mb; avail=10630.4921875Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000088\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001798\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2010.95703125Mb; avail=10630.24609375Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2015.140625Mb; avail=10626.0625Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000852\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2015.140625Mb; avail=10626.0625Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000034\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2015.140625Mb; avail=10626.0625Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000086\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001806\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2015.140625Mb; avail=10626.0625Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000785\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000035\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000110\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001874\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000748\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000031\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000087\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001616\n",
            "2024-09-20 17:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.8828125Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000774\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000034\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000087\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001814\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000758\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000036\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000084\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001726\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000758\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000030\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000082\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001652\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000801\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000030\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000080\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001691\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2019.3203125Mb; avail=10621.890625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2020.05859375Mb; avail=10621.15234375Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000854\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.05859375Mb; avail=10621.15234375Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000032\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.05859375Mb; avail=10621.15234375Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000412\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.002157\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.05859375Mb; avail=10621.15234375Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000739\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000029\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000079\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001599\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler order indices time: 0:00:00.000933\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler filter_by_size time: 0:00:00.000034\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] @batch_sampler batch_by_size time: 0:00:00.000089\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [None] per epoch batch_sampler set-up time: 0:00:00.001972\n",
            "2024-09-20 17:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=2020.55078125Mb; avail=10620.66015625Mb\n",
            "2024-09-20 17:00:54 | INFO | fairseq_cli.interactive | Total time: 4.236 seconds; translation time: 3.330\n"
          ]
        }
      ],
      "source": [
        "# model support the following languages : [as, bn, brx, gom, gu, hi, kn, ks, mai, ml, mni, mr, ne, or, pa, sa, sd, si, ta, te, ur]\n",
        "# -l = language code :: str\n",
        "# -i = name of the input file :: str\n",
        "# -b = beam size :: int\n",
        "# -n = best n candidates (b>=n) :: int\n",
        "# -r = rerank :: boolean (1 or 0)\n",
        "!bash transliterate_word.sh -l 'kn' -i 'source/source.txt' -b 10 -n 5 -r 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the Outputs"
      ],
      "metadata": {
        "id": "ex1dWcPKj6Kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB3XlPjGUmB7",
        "outputId": "807edcbf-e230-4d4e-c08d-6e6a36ec47ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bharat\t[ಭಾರತ್, ಭರತ್, ಭರಾತ್, ಬಾರತ್, ಭರಾಟ್]\n",
            "india\t[ಇಂಡಿಯಾ, ಇಂದಿಯಾ, ಇಂಡಿಯ, ಇಂಡ್ಯಾ, ಇಂಧಿಯಾ]\n",
            "hai\t[ಹೈ, ಹಾಯ್, ಹಾಯಿ, ಹೇ, ಹಯ್]\n",
            "b<unk>arat\t[ಬಾರತ್, ಬಾರಾತ್, ಬರತ್, ಬರಾತ್, ಬಾರಾಟ್]\n",
            "h<unk>i\t[ಹಿಯಾ, ಹಿಯ, ಹೀಯಾ, ಹಿಯ್, ಹಿಅ]\n",
            "haati\t[ಹಾತಿ, ಹಾಟಿ, ಹತಿ, ಹಟಿ, ಹಾತಿಯ]\n",
            "tuje\t[ತುಜೆ, ತೂಜೆ, ತುಜೇ, ಟುಜೆ, ಟೂಜೆ]\n",
            "<unk>\t[ಸ್, ಲ್, ವ್, ದ್, ಹ್]\n",
            "poloi\t[ಪೋಲೋಯಿ, ಪೊಲಾಯಿ, ಪೊಲೊಯಿ, ಪೊಲೋಯಿ, ಪೋಲೊಯಿ]\n",
            "<unk>anv\t[ಅನ್ವ, ಆಂವ್, ಅಂವ್, ಅಣ್ವ, ಅನ್ವ್]\n",
            "magnnem\t[ಮಗ್ನ್ನೆಂ, ಮಾಗ್ನ್ನೆಂ, ಮ್ಯಾಗ್ನೆಂ, ಮಗ್ನ್ನೆಮ್, ಮಗ್ನ್ನೇಂ]\n",
            "kortam\t[ಕೊರ್ಟಮ್, ಕೋರ್ಟಮ್, ಕೋರ್ಟಂ, ಕೊರ್ಟಂ, ಕೊರ್ತಮ್]\n",
            "hanv\t[ಹಾಂವ್, ಹಾನ್ವ್, ಹನ್ವ್, ಹಣ್ವ್, ಹಂವ್]\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cat output/final_transliteration.txt\n",
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indic-En setup\n"
      ],
      "metadata": {
        "id": "_TN7Fjfnqn2_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zKoMHkdq-YM"
      },
      "source": [
        "### Clone IndicXlit repository and download the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETuFFfWFq-YN"
      },
      "outputs": [],
      "source": [
        "# to setup and run indic-en command line inference)\n",
        "%cd /content/IndicXlit/inference/cli/indic-en\n",
        "\n",
        "# download the IndicXlit models from github release\n",
        "!wget https://github.com/AI4Bharat/IndicXlit/releases/download/v1.0/indicxlit-indic-en-v1.0.zip\n",
        "!unzip indicxlit-indic-en-v1.0.zip\n",
        "\n",
        "# download the Unigram dictionaries for reranking from github release\n",
        "!wget https://github.com/AI4Bharat/IndicXlit/releases/download/v1.0/word_prob_dicts_en.zip\n",
        "!unzip word_prob_dicts_en.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QngLbbirq-YQ"
      },
      "outputs": [],
      "source": [
        "# creating the dir for saving input files\n",
        "%mkdir source\n",
        "\n",
        "# creating the dir for saving output files\n",
        "%mkdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf8adT8_q-YR"
      },
      "source": [
        "### Saving the input words to source file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inserting space in between the characters\n",
        "s = 'भारत'\n",
        "' '.join(list(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r_UAtpu4rv-t",
        "outputId": "ddc86ee0-bcc4-40f3-f71a-081cb16456a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'भ ा र त'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imy_0674q-YR"
      },
      "outputs": [],
      "source": [
        "# Preparing a list of words, format : space separated roman characters in each line\n",
        "!echo \"भ ा र त\" >> source/source.txt\n",
        "!echo \"इ ं ड ि य ा\" >> source/source.txt\n",
        "!echo \"ह ै\" >> source/source.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input file would look like this\n",
        "```\n",
        "भ ा र त\n",
        "इ ं ड ि य ा\n",
        "ह ै\n",
        "````"
      ],
      "metadata": {
        "id": "pnu_fRRnq-YS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zfVKlSHq-YS"
      },
      "source": [
        "### Running the inference script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Uc9Oogq-YS"
      },
      "outputs": [],
      "source": [
        "# model support the following languages : [as, bn, brx, gom, gu, hi, kn, ks, mai, ml, mni, mr, ne, or, pa, sa, sd, si, ta, te, ur]\n",
        "# -l = language code :: str\n",
        "# -i = name of the input file :: str\n",
        "# -b = beam size :: int\n",
        "# -n = best n candidates (b>=n) :: int\n",
        "# -r = rerank :: boolean (1 or 0)\n",
        "!bash transliterate_word.sh -l 'hi' -i 'source/source.txt' -b 10 -n 5 -r 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the Outputs"
      ],
      "metadata": {
        "id": "xaLTeWgQq-YT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2569c3-7600-4809-b1a7-a3d83c244894",
        "id": "3atwFM0aq-YU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "भारत\t[bharat, bhaarat, bharath, bharata, bhaarut]\n",
            "इंडिया\t[india, indiya, indiyaa, indea, endia]\n",
            "है\t[hai, haye, haii, haie, haih]\n"
          ]
        }
      ],
      "source": [
        "!cat output/final_transliteration.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLxEms6OyI7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gG-S1hiFqgtO"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}